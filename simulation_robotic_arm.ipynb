{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Maintenance in the Robotic Arms Industry by applying Digital Twin Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main working file. It includes a function that simulates a robotic arm. During its operation, there will be irregularities. The applied algorithms will try to detect the irregularities, classify them as anomalies, and set further measures to pinpoint the problematic component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, make sure that all neccessary modules and programms are correctly installed. Also, I would advise to use Python 3.7.9, since I had troubles running parts of the code with other versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine learning\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "\n",
    "#azure\n",
    "from azure.digitaltwins.core import DigitalTwinsClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity import VisualStudioCodeCredential\n",
    "\n",
    "#additional installs\n",
    "import os\n",
    "import time\n",
    "\n",
    "#python scripts\n",
    "import anomaly_detection as ad\n",
    "import digital_twin_azure as dt\n",
    "import predictive_maintenance as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Azure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code connects your environment to the Azure platform. A browser window will pop up and you will be asked to login to your microsoft account. This information will then be stored for easy access of to the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk uses the credentials received by the previous step. It will build a service client which will be needed to update the Digital Twin, or run certain queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the URL of your Digital Twin instance on the Azure platzform\n",
    "url = \"SeleniumForest.api.weu.digitaltwins.azure.net\"\n",
    "\n",
    "#store the gathered credentials in a variable\n",
    "credential = DefaultAzureCredential()\n",
    "#create an instance of the Digital Twin Client\n",
    "#It can be resued later on\n",
    "global service_client\n",
    "service_client = DigitalTwinsClient(url, credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load the data set into a pandas datafram. Make sure that the Data repository exists within the set working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the CSV content in a dataframe\n",
    "df = pd.read_csv('./Data/right_arm.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will cover the pre-processing of the available dataframe. First, all blank spaces within the column names will be replaced by underlines. This will make working with the names a lot easier, as it allows for simple copy and paste shortcuts and avoid processing errors. Furthermore, the function extract_every_nth_row() can be used to reduce the proccessing time. It extracts every n´th row of the dataframe. Since one might run low computational resources, n can be adjusted to personal preferences or skipped entirely. \n",
    "\n",
    "Furthermore, there will  be a train and test split of all defined features and the target. Lastly, the index will be reseted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace blank spaces in the column names with '_'\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "#use only required columns\n",
    "df = df[['Norm_of_Cartesion_Linear_Momentum', 'Robot_Current', 'Tool_Current', 'Tool_Temperature', 'TCP_Force', 'Anomaly_State']]\n",
    "\n",
    "################################\n",
    "##make it smaller for testing###\n",
    "#since the dataset is quite big \n",
    "#one can keep this code block, to\n",
    "#shrink the dataset#############\n",
    "def extract_every_nth_row(df, n):\n",
    "    new_df = df.iloc[::n].copy()\n",
    "    return new_df\n",
    "\n",
    "n = 100\n",
    "\n",
    "df = extract_every_nth_row(df, n)\n",
    "################################\n",
    "################################\n",
    "\n",
    "#seperate the features and the target\n",
    "#X and y \n",
    "#features\n",
    "X = df[['Norm_of_Cartesion_Linear_Momentum', 'Robot_Current', 'Tool_Current', 'Tool_Temperature', 'TCP_Force']]\n",
    "#target\n",
    "y = df['Anomaly_State'] \n",
    "\n",
    "#Train & Test Split\n",
    "#for now the testsize is 20% of the total dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#reset the index to make the subsets iterateable again\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will focus on creating and running the simulation. The function start_machine() contains all previously created python scripts. By running start_machine() the function will simulate the startup of the robotic arm. Sensory data coming from teh simulation data set will be received. \n",
    "\n",
    "First, the code will make sure that the environment is correctly connected to the Azure instance. A browser window might pop-up and you will be asked to login with your personal credentials. Next, train_model() will be used to build a random forest prediction model. After that step is completed, simulated robotic arm will start sending sensory data to the digital environment. By running predict_model(), the algorithm will predict the current anomaly state for the machine given the pre-trained model. After that, the received prediction will be sent to the Digital Twin together with all received sensory data. Additionally, a dashboard-like plot will be displayed, showing all current workloads and the machines anomaly state. In case an anomaly is detected, an alarm will pop-up notifing the user. Furthermore, a SHAP bar plot will appear. This plot will show which component was relevant for the algorithm´s predictions. Domain-expert therefore, can take a closer look at the identified faulty components. \n",
    "\n",
    "This can be done untill the simulation data runs out, or the process has reached the predefined threshold (see \"set iterations for simulation run\").  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_machine(model_name: str, df, df_sim, index):\n",
    "        print('Connecting to the Azure platform...')\n",
    "        service_client = dt.connect_azure()\n",
    "        \n",
    "        print('Machine starting up...')\n",
    "        print('Training ML algorithm...')\n",
    "        rf = ad.train_model(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        print('Model has been trained')\n",
    "        \n",
    "        #set as global to calculate resulting scores\n",
    "        global y_predicted\n",
    "        \n",
    "        #set empty list to store predictions\n",
    "        global y_predicted_list\n",
    "        y_predicted_list = []\n",
    "\n",
    "        #set iterations for simulation run\n",
    "        while index < 5:\n",
    "                y_predicted = ad.predict_model(X_test, index, rf)\n",
    "                y_predicted_list.append(y_predicted)\n",
    "                dt.update_machine('RoboArm', X_test, index, y_predicted)\n",
    "                dt.plot_twin_state()\n",
    "                \n",
    "                if y_predicted == 1:\n",
    "                        print('Anomaly detected!')\n",
    "                else:\n",
    "                        print('No anomaly detected.')\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "        print('Simulation complete. Generating SHAP beeswarm and partial dependence plot...')\n",
    "        \n",
    "        #run function to generate beeswarm for all instances of X_test.\n",
    "        #also prints pdp for chosen feature and single instance with index 0.\n",
    "        pm.explain_prediction(X_train, y_train, X_test, 0)\n",
    "           \n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk will start the machine. By defining the initial index the starting point within the simulation data set can be chosen. In this case, the X_test set was used as the simulation dataset, giving the possibility to compare prediction results with actual anomalies.\n",
    "\n",
    "It is worth mentioning, that SHAP might have issues displaying the plot due to interdepenencies to other plot modules. For most cases, the shap.initjs() should fix this issue. However, if the issue still occurs I would recommend storing the plot as a jpg or png and open it manually on your machine instead of within the IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set starting index for simulation data set\n",
    "shap.initjs()\n",
    "\n",
    "start_machine('RoboArm', df, X_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Results\n",
    "The following code can be used to make working and evaluating the digital twin´s performance a bit easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test[:15], y_predicted_list)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "#Accuracy of simulation\n",
    "accuracy = accuracy_score(y_test[:15], y_predicted_list)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#Precision of simulation\n",
    "precision = precision_score (y_test[:15], y_predicted_list)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "#F1 Score of simualtion\n",
    "#f1_score = f1_score(y_test[:15], y_predicted_list, average='weighted')\n",
    "#print(\"F1 score:\", f1_score)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "        'y_test': y_test[:15], \n",
    "        'prediction_results': y_predicted_list\n",
    "    })\n",
    "comparison_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beeswarm Plot to visualize SHAP values during Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will print a SHAP beeswarm plot of the simulation data (X_test)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "positive_indices = np.where(y_predicted == 1)[0]\n",
    "shap_values_positive = shap_values[positive_indices, :, 1]\n",
    "\n",
    "shap.plots.beeswarm(shap_values_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Dependence Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code generates a SHAP partial dependence plot for a specific feature.\n",
    "#The feature can be chosen by changing the first function parameter.\n",
    "\n",
    "#select only positive entries \n",
    "selected_X_test = X_test.iloc[positive_indices, :]\n",
    "selected_X_test\n",
    "\n",
    "selected_y_test = np.where(y_test == 1)\n",
    "selected_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show partial dependence plot\n",
    "sample_ind=4\n",
    "shap.partial_dependence_plot(\n",
    "    \"Tool_Temperature\", model.predict, selected_X_test, model_expected_value=True,\n",
    "    feature_expected_value=True, ice=False,\n",
    "    shap_values=shap_values_positive[sample_ind:sample_ind+1,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.create_shap_beeswarm_all(X_train, y_train, X_test, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
